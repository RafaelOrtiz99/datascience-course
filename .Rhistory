x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
quantiles <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantils[x, probs = c(0.05, 0.95), na.rm = removeNA]
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
quantiles <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantiles[x, probs = c(0.05, 0.95), na.rm = removeNA]
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
quantiles <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantile[x, probs = c(0.05, 0.95), na.rm = removeNA]
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
qrts <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantile[x, probs = c(0.05, 0.95), na.rm = removeNA]
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
qrts <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantile(x, probs = c(0.05, 0.95), na.rm = removeNA)
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - h] <- caps[1]
x[x>qrts[2] + h] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
#Remplazar outliers
replace_outliers <- function(x, removeNA = TRUE){
qrts <- quantile(x, probs = c(0.25, 0.75), na.rm = removeNA)
caps <- quantile(x, probs = c(0.05, 0.95), na.rm = removeNA)
#Rango intercuartilico
iqr <- qrts[2] - qrts[1]
#1.5 veces el rango intercuartilico
H <- 1.5 * iqr
x[x<qrts[1] - H] <- caps[1]
x[x>qrts[2] + H] <- caps[2]
x
}
par(mfrow = c(1,2))
boxplot(ozone_data$pressure_height, main = "Presión con outliers")
capped_pressure_height <- replace_outliers(ozone_data$pressure_height)
boxplot(capped_pressure_height, main = "Presión sin outliers")
data <- read.csv('Recursos/r-course-master/data/tema2/auto-mpg.csv', header = T, stringsAsFactors = F)
#Categorizar la columna cylinders
data$cylinders <- factor(data$cylinders, levels = c(3,4,5,6,8),
labels = c('3cil', '4cil', '5cil', '6cil', '8cil'))
library(modeest) #Moda
library(raster) #quantiles, cv(coeficiente de variación)
library(moments) #asimetria, curtosis
X = data$mpg
#Media aritmetica
mean(X)
sum(X)/length(X)
#Media aritmetica
mean(X)
#Mediana
median(X)
#Moda
mfv(X)
library(modeest) #Moda
#Moda
mfv(X)
#Percentil o cuantil
quantile(X)
#Varianza
var(X)
#Desviación tipica
sd(X)
#Coeficiente de variación
cv(X)
library(raster) #quantiles, cv(coeficiente de variación)
#Coeficiente de variación
cv(X)
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
read.csv('~/Descargas/cuestionario.csv')
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
cuestionario <- read.csv('~/Descargas/cuestionario.csv')
View(cuestionario)
View(cuestionario)
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ',')
View(cuestionario)
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ',', header = FALSE)
View(cuestionario)
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = FALSE)
View(cuestionario)
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
#quitamos los numeros
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#cargamos las librerias correspondientes
library(tm)
library(NLP)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
#DISCURSO DE NO VIOLENCIA DE GANDHI
#Que palabras hay con mayor frecuencia en el discurso de Gandhi?
#nuestro texto lo guardamos en bloc de notas en formato txt
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = FALSE)
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
#quitamos los numeros
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
#Creamos matriz de letras
letras= TermDocumentMatrix(discurso)
findFreqTerms(letras, lowfreq=5)
matrix=as.matrix(letras)
#lo ordenamos y sumamos las letras de nuestra matriz
vector <- sort(rowSums(matrix),decreasing=TRUE)
#creamos la data con las palabras y su frecuencia
dataletras <- data.frame(word= names(vector),frequencia=vector)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS M?S FRECUENTES", ylab = "Frecuencia de palabras")
#instalamos todos los paquetes que necesitamos
install.packages("tm")
install.packages("tm")
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
#quitamos los numeros
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
#Creamos matriz de letras
letras= TermDocumentMatrix(discurso)
findFreqTerms(letras, lowfreq=5)
matrix=as.matrix(letras)
#lo ordenamos y sumamos las letras de nuestra matriz
vector <- sort(rowSums(matrix),decreasing=TRUE)
#creamos la data con las palabras y su frecuencia
dataletras <- data.frame(word= names(vector),frequencia=vector)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS M?S FRECUENTES", ylab = "Frecuencia de palabras")
############ GRAFICAMOS LA NUBE DE PALABRAS
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
#cargamos las libreris correspondientes
library(tm)
library(NLP)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
#quitamos los numeros
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
#Creamos matriz de letras
letras= TermDocumentMatrix(discurso)
findFreqTerms(letras, lowfreq=5)
matrix=as.matrix(letras)
#lo ordenamos y sumamos las letras de nuestra matriz
vector <- sort(rowSums(matrix),decreasing=TRUE)
#creamos la data con las palabras y su frecuencia
dataletras <- data.frame(word= names(vector),frequencia=vector)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS M?S FRECUENTES", ylab = "Frecuencia de palabras")
############ GRAFICAMOS LA NUBE DE PALABRAS
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(5, "Dark2"))
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(4, "Dark2"))
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(9, "Dark2"))
View(matrix)
View(matrix)
mstrix
matrix
View(texto)
View(matrix)
View(letras)
View(discurso)
View(dataletras)
dataletras
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(9, "Dark2"))
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = ddataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 6,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 6,
max.words=20, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS M?S FRECUENTES", ylab = "Frecuencia de palabras")
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud2(df, size=1.2)
install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(df, size=1.2)
wordcloud2(dataletras, size=1.2)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#en el centro la palabra mas importante,
wordcloud2(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
require(devtools)
install.packages('likert')
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = FALSE)
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = FALSE)
View(cuestionario)
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = T)
cuestionario <- read.csv('~/Descargas/cuestionario.csv', sep = ';', header = FALSE)
texto <- readLines("~/Descargas/respuestas.txt")
texto <- readLines("~/Descargas/1.txt")
l11 <- likert(texto)
library(likert)
l11 <- likert(texto)
l11 <- likert(data(texto))
dataset <- data(texto)
dataset <- data.frame(texto)
View(dataset)
l11 <- likert(dataset)
summary(l11)
l11 <- likert(dataset)
View(dataset)
l11 <- likert(dataset)
dataset <- data.frame(texto, indices)
View(dataset)
dataset <- data.frame(c(texto, indices))
View(dataset)
texto <- c(4,4,4)
indices <- c(1,2,3)
dataset <- data.frame(texto, indices)
View(cuestionario)
View(dataset)
dataset <- data.frame(indices, texto)
l11 <- likert(dataset)
names(items29) = c("Magazines", "Comic books", "Fiction", "Non-fiction books", "Newspapers")
names = c("Magazines", "Comic books", "Fiction", "Non-fiction books", "Newspapers")
l29 <- likert(names)
tema_graf <-theme_minimal() + theme(text = element_text(family = "serif"),
panel.grid.minor = element_blank(),
strip.background = element_rect(fill = "#EBEBEB", colour = NA),
legend.position = "none",
legend.box.background = element_rect(fill = "#EBEBEB", colour = NA))
resultados <- read.csv('~/Descargas/cuestionario.csv', stringsAsFactors = F, fileEncoding = "latin1") %>%
tbl_df()
resultados <- read.csv('~/Descargas/cuestionario.csv', stringsAsFactors = F, fileEncoding = "latin1")
View(resultados)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
#cargamos las libreris correspondientes
library(tm)
library(NLP)
library(SnowballC)
library(RColorBrewer)
library(wordcloud2)
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
#Creamos matriz de letras
letras= TermDocumentMatrix(discurso)
findFreqTerms(letras, lowfreq=5)
matrix=as.matrix(letras)
vector <- sort(rowSums(matrix),decreasing=TRUE)
dataletras <- data.frame(word= names(vector),frequencia=vector)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS M?S FRECUENTES", ylab = "Frecuencia de palabras")
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
wordcloud2(dataletras, size=1.2)
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.5,
colors=brewer.pal(8, "Dark2"))
ggplot(subdf, aes(label = word,
size=freq,
color = factor(sample.int(10, nrow(subdf), replace =T)))) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal()
ggplot(subdf, aes(label = word,
size=freq,
color = factor(sample.int(10, nrow(dataletras), replace =T)))) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal()
ggplot(dataletras, aes(label = word,
size=freq,
color = factor(sample.int(10, nrow(dataletras), replace =T)))) +
geom_text_wordcloud() +
scale_size_area(max_size = 10) +
theme_minimal()
wordcloud2(dataletras, size=1.2)
wordcloud2(dataletras, size=1.2, ellipticity = 0.1)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS MÁS FRECUENTES", ylab = "Frecuencia de palabras")
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS MÁS FRECUENTES", ylab = "Frecuencia de palabras")
texto <- readLines("~/Descargas/respuestas.txt")
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS MÁS FRECUENTES", ylab = "Frecuencia de palabras")
texto <- readLines("~/Descargas/respuestas.txt")
texto = iconv(texto, to="ASCII//TRANSLIT")
texto = Corpus(VectorSource(texto))
#ponemos todos los datos a minuscula (A!=a)
discurso=tm_map(texto, tolower)
#quitamos los espacios en blanco
discurso =tm_map(discurso, stripWhitespace)
#quitamos la puntuacion
discurso = tm_map(discurso, removePunctuation)
discurso = tm_map(discurso, removeNumbers)
#mostramos palabras vacias y genericas
stopwords("spanish")
#quitamos palabras genericas
discurso=tm_map(discurso, removeWords,stopwords("spanish"))
#Creamos matriz de letras
letras= TermDocumentMatrix(discurso)
findFreqTerms(letras, lowfreq=5)
matrix=as.matrix(letras)
vector <- sort(rowSums(matrix),decreasing=TRUE)
dataletras <- data.frame(word= names(vector),frequencia=vector)
barplot(dataletras[1:10,]$freq, las = 2, names.arg = dataletras[1:10,]$word,
col ="blue", main ="PALABRAS MÁS FRECUENTES", ylab = "Frecuencia de palabras")
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
wordcloud2(dataletras, size=1.2, ellipticity = 0.1)
wordcloud2(dataletras, size=1.2, ellipticity = 0.5)
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70)
#en el centro la palabra mas importante,
wordcloud(words = dataletras$word, freq = dataletras$freq, min.freq = 2,
max.words=70, random.order=FALSE, rot.per=0.2,
colors=brewer.pal(8, "Dark2"))
wordcloud2(dataletras, size=1.2, ellipticity = 0.5)
wordcloud2(dataletras, size=1.2, ellipticity = 0.9)
wordcloud2(dataletras, size=1.2, ellipticity = 0.9)
wordcloud2(dataletras, size=1.2, ellipticity = 0.5)
